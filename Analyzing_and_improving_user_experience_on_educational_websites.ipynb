{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfHeTzZYnrbi20hVDXwyCC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Cyber-Attack-Detection/blob/main/Analyzing_and_improving_user_experience_on_educational_websites.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "path = kagglehub.dataset_download(\"rabieelkharoua/predict-online-course-engagement-dataset\")\n",
        "print(\"Dataset Path:\", path)\n",
        "\n",
        "df = pd.read_csv(f\"{path}/online_course_engagement_data.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: EXPLORATORY DATA ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(f\"Target Distribution:\\n{df['CourseCompletion'].value_counts()}\")\n",
        "print(f\"Target Balance: {df['CourseCompletion'].mean():.2%} completed\")\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
        "fig.suptitle('Exploratory Data Analysis - Feature Distributions', fontsize=16, fontweight='bold')\n",
        "\n",
        "numeric_features = ['TimeSpentOnCourse', 'NumberOfVideosWatched', 'NumberOfQuizzesTaken',\n",
        "                    'QuizScores', 'CompletionRate']\n",
        "\n",
        "for idx, col in enumerate(numeric_features):\n",
        "    ax = axes[idx//3, idx%3]\n",
        "    for completion in [0, 1]:\n",
        "        data = df[df['CourseCompletion'] == completion][col]\n",
        "        ax.hist(data, bins=30, alpha=0.6, label=f'Completed={completion}')\n",
        "    ax.set_title(f'{col}')\n",
        "    ax.set_xlabel(col)\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.legend()\n",
        "\n",
        "axes[1, 2].pie(df['CourseCategory'].value_counts(), labels=df['CourseCategory'].value_counts().index,\n",
        "               autopct='%1.1f%%', startangle=90)\n",
        "axes[1, 2].set_title('Course Category Distribution')\n",
        "\n",
        "axes[2, 0].pie(df['DeviceType'].value_counts(), labels=['Mobile', 'Desktop'], autopct='%1.1f%%')\n",
        "axes[2, 0].set_title('Device Type Distribution')\n",
        "\n",
        "completion_by_category = df.groupby('CourseCategory')['CourseCompletion'].mean().sort_values()\n",
        "axes[2, 1].barh(completion_by_category.index, completion_by_category.values)\n",
        "axes[2, 1].set_xlabel('Completion Rate')\n",
        "axes[2, 1].set_title('Completion by Category')\n",
        "\n",
        "completion_by_device = df.groupby('DeviceType')['CourseCompletion'].mean()\n",
        "axes[2, 2].bar(['Mobile', 'Desktop'], completion_by_device.values, color=['skyblue', 'coral'])\n",
        "axes[2, 2].set_ylabel('Completion Rate')\n",
        "axes[2, 2].set_title('Completion by Device')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('eda_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: eda_distributions.png\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Advanced EDA - Correlations & Relationships', fontsize=16, fontweight='bold')\n",
        "\n",
        "corr_matrix = df[numeric_features + ['CourseCompletion']].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=axes[0, 0], square=True)\n",
        "axes[0, 0].set_title('Feature Correlation Matrix')\n",
        "\n",
        "scatter_features = [['TimeSpentOnCourse', 'CompletionRate'],\n",
        "                    ['NumberOfVideosWatched', 'QuizScores']]\n",
        "for idx, (feat1, feat2) in enumerate(scatter_features):\n",
        "    ax = axes[idx, 1]\n",
        "    for completion in [0, 1]:\n",
        "        data = df[df['CourseCompletion'] == completion]\n",
        "        ax.scatter(data[feat1], data[feat2], alpha=0.4, label=f'Completed={completion}')\n",
        "    ax.set_xlabel(feat1)\n",
        "    ax.set_ylabel(feat2)\n",
        "    ax.set_title(f'{feat1} vs {feat2}')\n",
        "    ax.legend()\n",
        "\n",
        "df.boxplot(column='CompletionRate', by='CourseCategory', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Completion Rate by Category')\n",
        "axes[1, 0].set_xlabel('Category')\n",
        "plt.suptitle('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('eda_advanced.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: eda_advanced.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: FEATURE ENGINEERING & PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df_processed = df.copy()\n",
        "\n",
        "df_processed['EngagementScore'] = (\n",
        "    df_processed['TimeSpentOnCourse'] * 0.3 +\n",
        "    df_processed['NumberOfVideosWatched'] * 2 +\n",
        "    df_processed['NumberOfQuizzesTaken'] * 3 +\n",
        "    df_processed['CompletionRate'] * 0.5\n",
        ")\n",
        "\n",
        "df_processed['QuizPerformance'] = df_processed['QuizScores'] * df_processed['NumberOfQuizzesTaken']\n",
        "df_processed['VideoEngagement'] = df_processed['NumberOfVideosWatched'] / (df_processed['TimeSpentOnCourse'] + 1)\n",
        "df_processed['QuizDensity'] = df_processed['NumberOfQuizzesTaken'] / (df_processed['TimeSpentOnCourse'] + 1)\n",
        "df_processed['AvgTimePerVideo'] = df_processed['TimeSpentOnCourse'] / (df_processed['NumberOfVideosWatched'] + 1)\n",
        "\n",
        "le_category = LabelEncoder()\n",
        "df_processed['CourseCategory_encoded'] = le_category.fit_transform(df_processed['CourseCategory'])\n",
        "\n",
        "print(f\"\\nNew Features Created: {['EngagementScore', 'QuizPerformance', 'VideoEngagement', 'QuizDensity', 'AvgTimePerVideo']}\")\n",
        "print(f\"Encoded Categories: {list(le_category.classes_)}\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "fig.suptitle('Feature Engineering Impact', fontsize=16, fontweight='bold')\n",
        "\n",
        "new_features = ['EngagementScore', 'QuizPerformance', 'VideoEngagement', 'QuizDensity', 'AvgTimePerVideo']\n",
        "for idx, feat in enumerate(new_features):\n",
        "    ax = axes[idx//3, idx%3]\n",
        "    for completion in [0, 1]:\n",
        "        data = df_processed[df_processed['CourseCompletion'] == completion][feat]\n",
        "        ax.hist(data, bins=30, alpha=0.6, label=f'Completed={completion}')\n",
        "    ax.set_title(f'New Feature: {feat}')\n",
        "    ax.set_xlabel(feat)\n",
        "    ax.legend()\n",
        "\n",
        "feature_importance_preview = df_processed[new_features + ['CourseCompletion']].corr()['CourseCompletion'].drop('CourseCompletion').sort_values()\n",
        "axes[1, 2].barh(feature_importance_preview.index, feature_importance_preview.values)\n",
        "axes[1, 2].set_xlabel('Correlation with Target')\n",
        "axes[1, 2].set_title('New Features Correlation')\n",
        "axes[1, 2].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('preprocessing.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: preprocessing.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: MODEL TRAINING WITH HYPERPARAMETER TUNING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "feature_cols = ['TimeSpentOnCourse', 'NumberOfVideosWatched', 'NumberOfQuizzesTaken',\n",
        "                'QuizScores', 'CompletionRate', 'DeviceType', 'CourseCategory_encoded',\n",
        "                'EngagementScore', 'QuizPerformance', 'VideoEngagement', 'QuizDensity', 'AvgTimePerVideo']\n",
        "\n",
        "X = df_processed[feature_cols]\n",
        "y = df_processed['CourseCompletion']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "models = {\n",
        "    'RandomForest': RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=10,\n",
        "        min_samples_split=20,\n",
        "        min_samples_leaf=10,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'GradientBoosting': GradientBoostingClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        min_samples_split=20,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'XGBoost': XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        min_child_weight=5,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1])\n",
        "    )\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
        "    train_score = model.score(X_train_scaled, y_train)\n",
        "    test_score = model.score(X_test_scaled, y_test)\n",
        "\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    results[name] = {\n",
        "        'train': train_score,\n",
        "        'test': test_score,\n",
        "        'cv_mean': cv_scores.mean(),\n",
        "        'cv_std': cv_scores.std(),\n",
        "        'auc': auc_score,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "    print(f\"  Train Accuracy: {train_score:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_score:.4f}\")\n",
        "    print(f\"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "    print(f\"  AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "best_model_name = max(results, key=lambda x: results[x]['auc'])\n",
        "best_model = results[best_model_name]['model']\n",
        "print(f\"\\n✓ Best Model: {best_model_name} (AUC: {results[best_model_name]['auc']:.4f})\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Model Training & Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "model_names = list(results.keys())\n",
        "metrics = ['train', 'test', 'cv_mean', 'auc']\n",
        "metric_labels = ['Train Acc', 'Test Acc', 'CV Score', 'AUC']\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.2\n",
        "\n",
        "for i, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
        "    values = [results[m][metric] for m in model_names]\n",
        "    axes[0, 0].bar(x + i*width, values, width, label=label, alpha=0.8)\n",
        "\n",
        "axes[0, 0].set_xlabel('Models')\n",
        "axes[0, 0].set_ylabel('Score')\n",
        "axes[0, 0].set_title('Model Performance Comparison')\n",
        "axes[0, 0].set_xticks(x + width*1.5)\n",
        "axes[0, 0].set_xticklabels(model_names)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False).head(10)\n",
        "\n",
        "    axes[0, 1].barh(range(len(feature_importance)), feature_importance['importance'])\n",
        "    axes[0, 1].set_yticks(range(len(feature_importance)))\n",
        "    axes[0, 1].set_yticklabels(feature_importance['feature'])\n",
        "    axes[0, 1].set_xlabel('Importance')\n",
        "    axes[0, 1].set_title('Top 10 Feature Importances')\n",
        "    axes[0, 1].invert_yaxis()\n",
        "\n",
        "overfitting = [(results[m]['train'] - results[m]['test']) for m in model_names]\n",
        "colors = ['red' if gap > 0.1 else 'orange' if gap > 0.05 else 'green' for gap in overfitting]\n",
        "axes[1, 0].bar(model_names, overfitting, color=colors, alpha=0.7)\n",
        "axes[1, 0].set_ylabel('Train - Test Gap')\n",
        "axes[1, 0].set_title('Overfitting Analysis')\n",
        "axes[1, 0].axhline(y=0.05, color='orange', linestyle='--', alpha=0.5, label='Threshold')\n",
        "axes[1, 0].axhline(y=0.1, color='red', linestyle='--', alpha=0.5)\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "cv_means = [results[m]['cv_mean'] for m in model_names]\n",
        "cv_stds = [results[m]['cv_std'] for m in model_names]\n",
        "axes[1, 1].bar(model_names, cv_means, yerr=cv_stds, capsize=10, alpha=0.7)\n",
        "axes[1, 1].set_ylabel('CV Score')\n",
        "axes[1, 1].set_title('Cross-Validation Scores')\n",
        "axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model_training.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: model_training.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: POST-PROCESSING & PREDICTION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "y_pred_proba = best_model.predict_proba(X_test_scaled)\n",
        "\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Actual': y_test.values,\n",
        "    'Predicted': y_pred,\n",
        "    'Prob_Incomplete': y_pred_proba[:, 0],\n",
        "    'Prob_Complete': y_pred_proba[:, 1],\n",
        "    'Confidence': y_pred_proba.max(axis=1)\n",
        "})\n",
        "\n",
        "predictions_df['Correct'] = (predictions_df['Actual'] == predictions_df['Predicted']).astype(int)\n",
        "predictions_df['Error_Type'] = predictions_df.apply(\n",
        "    lambda x: 'Correct' if x['Correct'] else ('False Positive' if x['Predicted'] == 1 else 'False Negative'),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(f\"\\nPrediction Accuracy: {predictions_df['Correct'].mean():.4f}\")\n",
        "print(f\"Average Confidence: {predictions_df['Confidence'].mean():.4f}\")\n",
        "print(f\"\\nError Distribution:\")\n",
        "print(predictions_df['Error_Type'].value_counts())\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Prediction Analysis & Error Patterns', fontsize=16, fontweight='bold')\n",
        "\n",
        "axes[0, 0].hist(predictions_df['Confidence'], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(predictions_df['Confidence'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
        "axes[0, 0].set_xlabel('Prediction Confidence')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Confidence Distribution')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "error_types = predictions_df['Error_Type'].value_counts()\n",
        "colors_map = {'Correct': 'green', 'False Positive': 'orange', 'False Negative': 'red'}\n",
        "colors = [colors_map[et] for et in error_types.index]\n",
        "axes[0, 1].bar(error_types.index, error_types.values, color=colors, alpha=0.7)\n",
        "axes[0, 1].set_ylabel('Count')\n",
        "axes[0, 1].set_title('Prediction Error Analysis')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "for error_type in ['Correct', 'False Positive', 'False Negative']:\n",
        "    if error_type in predictions_df['Error_Type'].values:\n",
        "        data = predictions_df[predictions_df['Error_Type'] == error_type]['Confidence']\n",
        "        axes[1, 0].hist(data, bins=20, alpha=0.5, label=error_type)\n",
        "axes[1, 0].set_xlabel('Confidence')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Confidence by Error Type')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "thresholds = np.arange(0.3, 0.8, 0.05)\n",
        "accuracies = []\n",
        "for thresh in thresholds:\n",
        "    pred_at_thresh = (predictions_df['Prob_Complete'] >= thresh).astype(int)\n",
        "    acc = (pred_at_thresh == predictions_df['Actual']).mean()\n",
        "    accuracies.append(acc)\n",
        "\n",
        "axes[1, 1].plot(thresholds, accuracies, marker='o', linewidth=2)\n",
        "axes[1, 1].axvline(0.5, color='red', linestyle='--', alpha=0.5, label='Default')\n",
        "axes[1, 1].set_xlabel('Decision Threshold')\n",
        "axes[1, 1].set_ylabel('Accuracy')\n",
        "axes[1, 1].set_title('Threshold Optimization')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('post_processing.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: post_processing.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 5: COMPREHENSIVE EVALUATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(\"\\nClassification Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "print(f\"MCC:       {mcc:.4f}\")\n",
        "print(f\"AUC-ROC:   {results[best_model_name]['auc']:.4f}\")\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Not Completed', 'Completed']))\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Comprehensive Model Evaluation', fontsize=16, fontweight='bold')\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
        "            xticklabels=['Not Complete', 'Complete'],\n",
        "            yticklabels=['Not Complete', 'Complete'])\n",
        "axes[0, 0].set_xlabel('Predicted')\n",
        "axes[0, 0].set_ylabel('Actual')\n",
        "axes[0, 0].set_title('Confusion Matrix')\n",
        "\n",
        "metrics_dict = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1,\n",
        "    'MCC': mcc,\n",
        "    'AUC': results[best_model_name]['auc']\n",
        "}\n",
        "bars = axes[0, 1].bar(metrics_dict.keys(), metrics_dict.values(),\n",
        "                      color=plt.cm.viridis(np.linspace(0, 1, len(metrics_dict))))\n",
        "axes[0, 1].set_ylabel('Score')\n",
        "axes[0, 1].set_title('Performance Metrics Summary')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].set_ylim([0, 1.1])\n",
        "for bar, val in zip(bars, metrics_dict.values()):\n",
        "    height = bar.get_height()\n",
        "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                    f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_proba[:, 1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "axes[0, 2].plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.3f})', linewidth=2)\n",
        "axes[0, 2].plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
        "axes[0, 2].fill_between(fpr, tpr, alpha=0.2)\n",
        "axes[0, 2].set_xlabel('False Positive Rate')\n",
        "axes[0, 2].set_ylabel('True Positive Rate')\n",
        "axes[0, 2].set_title('ROC Curve')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=axes[1, 0],\n",
        "            xticklabels=['Not Complete', 'Complete'],\n",
        "            yticklabels=['Not Complete', 'Complete'])\n",
        "axes[1, 0].set_xlabel('Predicted')\n",
        "axes[1, 0].set_ylabel('Actual')\n",
        "axes[1, 0].set_title('Normalized Confusion Matrix')\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba[:, 1])\n",
        "axes[1, 1].plot(recall_curve, precision_curve, linewidth=2, label='PR Curve')\n",
        "axes[1, 1].fill_between(recall_curve, precision_curve, alpha=0.2)\n",
        "axes[1, 1].set_xlabel('Recall')\n",
        "axes[1, 1].set_ylabel('Precision')\n",
        "axes[1, 1].set_title('Precision-Recall Curve')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].legend()\n",
        "\n",
        "sample_size = min(50, len(y_test))\n",
        "sample_indices = np.random.choice(len(y_test), sample_size, replace=False)\n",
        "axes[1, 2].scatter(range(sample_size), y_test.iloc[sample_indices],\n",
        "                   label='Actual', alpha=0.6, s=100)\n",
        "axes[1, 2].scatter(range(sample_size), y_pred[sample_indices],\n",
        "                   label='Predicted', alpha=0.6, s=100, marker='x')\n",
        "axes[1, 2].set_xlabel('Sample Index')\n",
        "axes[1, 2].set_ylabel('Class')\n",
        "axes[1, 2].set_title(f'Actual vs Predicted (Sample of {sample_size})')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('evaluation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"✓ Saved: evaluation.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL REPORT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"Model Configuration: Balanced with regularization\")\n",
        "print(f\"\\nPerformance Summary:\")\n",
        "print(f\"  - Test Accuracy: {results[best_model_name]['test']:.4f}\")\n",
        "print(f\"  - AUC-ROC Score: {results[best_model_name]['auc']:.4f}\")\n",
        "print(f\"  - Cross-Val Score: {results[best_model_name]['cv_mean']:.4f} (+/- {results[best_model_name]['cv_std']:.4f})\")\n",
        "print(f\"  - F1-Score: {f1:.4f}\")\n",
        "print(f\"\\nDataset Info:\")\n",
        "print(f\"  - Total Samples: {len(df)}\")\n",
        "print(f\"  - Features Used: {len(feature_cols)} (including engineered features)\")\n",
        "print(f\"  - Train/Test Split: {len(X_train)}/{len(X_test)}\")\n",
        "print(f\"  - Class Balance: {y.mean():.2%} completed\")\n",
        "print(f\"\\nTop 3 Most Important Features:\")\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    top_features = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False).head(3)\n",
        "    for _, row in top_features.iterrows():\n",
        "        print(f\"  - {row['feature']}: {row['importance']:.4f}\")\n",
        "print(\"\\n✓ Analysis Complete - All Visualizations Saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fpPYN3MbDUSh",
        "outputId": "9a134d20-807a-4287-a326-b46e5995d3a8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3516110004.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rabieelkharoua/predict-online-course-engagement-dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset Path:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/datasets.py\u001b[0m in \u001b[0;36mdataset_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_dataset_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading Dataset: {h.to_url()} ...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mEXTRA_CONSOLE_BLOCK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle, path, force_download)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mSome\u001b[0m \u001b[0mcases\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mmight\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompetition\u001b[0m \u001b[0mdatasource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAPI\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbased\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/colab_cache_resolver.py\u001b[0m in \u001b[0;36m_resolve\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    127\u001b[0m             )\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mapi_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColabClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         data = {\n\u001b[1;32m    131\u001b[0m             \u001b[0;34m\"owner\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/clients.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mColabEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_kaggle_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Content-type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/config.py\u001b[0m in \u001b[0;36mget_kaggle_credentials\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0musername\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreds_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCREDENTIALS_JSON_USERNAME\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreds_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCREDENTIALS_JSON_KEY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             )\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_in_colab_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcolab_secret\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mget_colab_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mKaggleApiCredentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolab_secret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolab_secret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/kagglehub/config.py\u001b[0m in \u001b[0;36mget_colab_credentials\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOLAB_SECRET_USERNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_normalize_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOLAB_SECRET_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0musername\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;31m# thread-safe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0m_userdata_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     resp = _message.blocking_request(\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;34m'GetSecret'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Import Libraries and Load Dataset\n"
      ],
      "metadata": {
        "id": "_EyCzNx-IBC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "path = kagglehub.dataset_download(\"rabieelkharoua/predict-online-course-engagement-dataset\")\n",
        "df = pd.read_csv(f\"{path}/online_course_engagement_data.csv\")"
      ],
      "metadata": {
        "id": "H9GkAC7zDVRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Exploratory Data Analysis\n"
      ],
      "metadata": {
        "id": "W6CxEJ7yJENm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "print(f\"Target Distribution:\\n{df['CourseCompletion'].value_counts()}\")\n",
        "print(f\"Target Balance: {df['CourseCompletion'].mean():.2%} completed\")\n",
        "\n",
        "# Create figure with better layout\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Feature distributions\n",
        "numeric_features = ['TimeSpentOnCourse', 'NumberOfVideosWatched', 'NumberOfQuizzesTaken',\n",
        "                    'QuizScores', 'CompletionRate']\n",
        "\n",
        "for idx, col in enumerate(numeric_features):\n",
        "    ax = fig.add_subplot(gs[idx//3, idx%3])\n",
        "    sns.histplot(data=df, x=col, hue='CourseCompletion', kde=True, ax=ax)\n",
        "    ax.set_title(f'{col} Distribution')\n",
        "\n",
        "# Course category distribution\n",
        "ax = fig.add_subplot(gs[1, 2])\n",
        "df['CourseCategory'].value_counts().plot.pie(autopct='%1.1f%%', ax=ax)\n",
        "ax.set_title('Course Category Distribution')\n",
        "\n",
        "# Device type distribution\n",
        "ax = fig.add_subplot(gs[2, 0])\n",
        "df['DeviceType'].value_counts().plot.pie(labels=['Mobile', 'Desktop'], autopct='%1.1f%%', ax=ax)\n",
        "ax.set_title('Device Type Distribution')\n",
        "\n",
        "# Completion by category\n",
        "ax = fig.add_subplot(gs[2, 1])\n",
        "completion_by_category = df.groupby('CourseCategory')['CourseCompletion'].mean().sort_values()\n",
        "completion_by_category.plot.barh(ax=ax)\n",
        "ax.set_title('Completion Rate by Category')\n",
        "\n",
        "# Completion by device\n",
        "ax = fig.add_subplot(gs[2, 2])\n",
        "completion_by_device = df.groupby('DeviceType')['CourseCompletion'].mean()\n",
        "completion_by_device.plot.bar(ax=ax, color=['skyblue', 'coral'])\n",
        "ax.set_title('Completion Rate by Device')\n",
        "\n",
        "# Correlation matrix\n",
        "ax = fig.add_subplot(gs[3, :])\n",
        "corr_matrix = df[numeric_features + ['CourseCompletion']].corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)\n",
        "ax.set_title('Feature Correlation Matrix')\n",
        "\n",
        "plt.savefig('eda_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "81gakEjwJBWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Feature Engineering\n"
      ],
      "metadata": {
        "id": "q-4qKrehJtee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_processed = df.copy()\n",
        "\n",
        "# Create new features\n",
        "df_processed['EngagementScore'] = (\n",
        "    df_processed['TimeSpentOnCourse'] * 0.3 +\n",
        "    df_processed['NumberOfVideosWatched'] * 2 +\n",
        "    df_processed['NumberOfQuizzesTaken'] * 3 +\n",
        "    df_processed['CompletionRate'] * 0.5\n",
        ")\n",
        "\n",
        "df_processed['QuizPerformance'] = df_processed['QuizScores'] * df_processed['NumberOfQuizzesTaken']\n",
        "df_processed['VideoEngagement'] = df_processed['NumberOfVideosWatched'] / (df_processed['TimeSpentOnCourse'] + 1)\n",
        "df_processed['QuizDensity'] = df_processed['NumberOfQuizzesTaken'] / (df_processed['TimeSpentOnCourse'] + 1)\n",
        "df_processed['AvgTimePerVideo'] = df_processed['TimeSpentOnCourse'] / (df_processed['NumberOfVideosWatched'] + 1)\n",
        "\n",
        "# Encode categorical features\n",
        "le_category = LabelEncoder()\n",
        "df_processed['CourseCategory_encoded'] = le_category.fit_transform(df_processed['CourseCategory'])\n",
        "\n",
        "# Visualize new features\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "new_features = ['EngagementScore', 'QuizPerformance', 'VideoEngagement', 'QuizDensity', 'AvgTimePerVideo']\n",
        "for idx, feat in enumerate(new_features):\n",
        "    ax = fig.add_subplot(gs[idx//3, idx%3])\n",
        "    sns.histplot(data=df_processed, x=feat, hue='CourseCompletion', kde=True, ax=ax)\n",
        "    ax.set_title(f'{feat} Distribution')\n",
        "\n",
        "# Feature importance preview\n",
        "ax = fig.add_subplot(gs[1, 2])\n",
        "feature_importance_preview = df_processed[new_features + ['CourseCompletion']].corr()['CourseCompletion'].drop('CourseCompletion').sort_values()\n",
        "feature_importance_preview.plot.barh(ax=ax)\n",
        "ax.set_title('Feature Correlation with Target')\n",
        "\n",
        "plt.savefig('feature_engineering.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rm_AtmmzJB51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Model Training\n"
      ],
      "metadata": {
        "id": "6WOEwuVWJ9Hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "feature_cols = ['TimeSpentOnCourse', 'NumberOfVideosWatched', 'NumberOfQuizzesTaken',\n",
        "                'QuizScores', 'CompletionRate', 'DeviceType', 'CourseCategory_encoded',\n",
        "                'EngagementScore', 'QuizPerformance', 'VideoEngagement', 'QuizDensity', 'AvgTimePerVideo']\n",
        "\n",
        "X = df_processed[feature_cols]\n",
        "y = df_processed['CourseCompletion']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'RandomForest': RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=10,\n",
        "        min_samples_split=20,\n",
        "        min_samples_leaf=10,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'GradientBoosting': GradientBoostingClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        min_samples_split=20,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'XGBoost': XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=5,\n",
        "        min_child_weight=5,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss',\n",
        "        scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1])\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
        "    train_score = model.score(X_train_scaled, y_train)\n",
        "    test_score = model.score(X_test_scaled, y_test)\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    results[name] = {\n",
        "        'train': train_score,\n",
        "        'test': test_score,\n",
        "        'cv_mean': cv_scores.mean(),\n",
        "        'cv_std': cv_scores.std(),\n",
        "        'auc': auc_score,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "# Find best model\n",
        "best_model_name = max(results, key=lambda x: results[x]['auc'])\n",
        "best_model = results[best_model_name]['model']\n",
        "\n",
        "# Visualize results\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Model comparison\n",
        "ax = fig.add_subplot(gs[0, 0])\n",
        "model_names = list(results.keys())\n",
        "metrics = ['train', 'test', 'cv_mean', 'auc']\n",
        "metric_labels = ['Train Acc', 'Test Acc', 'CV Score', 'AUC']\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.2\n",
        "\n",
        "for i, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
        "    values = [results[m][metric] for m in model_names]\n",
        "    ax.bar(x + i*width, values, width, label=label, alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance Comparison')\n",
        "ax.set_xticks(x + width*1.5)\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.legend()\n",
        "\n",
        "# Feature importance\n",
        "ax = fig.add_subplot(gs[0, 1])\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False).head(10)\n",
        "\n",
        "    sns.barplot(data=feature_importance, x='importance', y='feature', ax=ax)\n",
        "    ax.set_title('Top 10 Feature Importances')\n",
        "\n",
        "# Overfitting analysis\n",
        "ax = fig.add_subplot(gs[1, 0])\n",
        "overfitting = [(results[m]['train'] - results[m]['test']) for m in model_names]\n",
        "colors = ['red' if gap > 0.1 else 'orange' if gap > 0.05 else 'green' for gap in overfitting]\n",
        "sns.barplot(x=model_names, y=overfitting, palette=colors, ax=ax)\n",
        "ax.set_title('Overfitting Analysis')\n",
        "ax.axhline(y=0.05, color='orange', linestyle='--', alpha=0.5)\n",
        "ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Cross-validation scores\n",
        "ax = fig.add_subplot(gs[1, 1])\n",
        "cv_means = [results[m]['cv_mean'] for m in model_names]\n",
        "cv_stds = [results[m]['cv_std'] for m in model_names]\n",
        "\n",
        "# Plot the bars without yerr first\n",
        "sns.barplot(x=model_names, y=cv_means, ax=ax)\n",
        "\n",
        "# Add error bars manually\n",
        "# The x-coordinates for the bars in seaborn barplot (with categorical x) are typically 0, 1, 2...\n",
        "for i, (mean_val, std_val) in enumerate(zip(cv_means, cv_stds)):\n",
        "    ax.errorbar(i, mean_val, yerr=std_val, fmt='none', c='black', capsize=5)\n",
        "\n",
        "ax.set_title('Cross-Validation Scores')\n",
        "\n",
        "plt.savefig('model_training.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Pf0pIZceKDt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Model Evaluation\n"
      ],
      "metadata": {
        "id": "FoFmZLdaSaqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "y_pred_proba = best_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Create evaluation visualizations\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Confusion matrix\n",
        "ax = fig.add_subplot(gs[0, 0])\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "            xticklabels=['Not Complete', 'Complete'],\n",
        "            yticklabels=['Not Complete', 'Complete'])\n",
        "ax.set_title('Confusion Matrix')\n",
        "\n",
        "# Performance metrics\n",
        "ax = fig.add_subplot(gs[0, 1])\n",
        "metrics_dict = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1,\n",
        "    'MCC': mcc,\n",
        "    'AUC': results[best_model_name]['auc']\n",
        "}\n",
        "sns.barplot(x=list(metrics_dict.keys()), y=list(metrics_dict.values()), ax=ax)\n",
        "ax.set_title('Performance Metrics')\n",
        "ax.set_ylim([0, 1.1])\n",
        "\n",
        "# ROC curve\n",
        "ax = fig.add_subplot(gs[0, 2])\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "ax.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc:.3f})')\n",
        "ax.plot([0, 1], [0, 1], 'k--')\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curve')\n",
        "ax.legend()\n",
        "\n",
        "# Normalized confusion matrix\n",
        "ax = fig.add_subplot(gs[1, 0])\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Greens', ax=ax,\n",
        "            xticklabels=['Not Complete', 'Complete'],\n",
        "            yticklabels=['Not Complete', 'Complete'])\n",
        "ax.set_title('Normalized Confusion Matrix')\n",
        "\n",
        "# Precision-Recall curve\n",
        "ax = fig.add_subplot(gs[1, 1])\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba[:, 1])\n",
        "ax.plot(recall_curve, precision_curve)\n",
        "ax.set_xlabel('Recall')\n",
        "ax.set_ylabel('Precision')\n",
        "ax.set_title('Precision-Recall Curve')\n",
        "\n",
        "# Prediction confidence\n",
        "ax = fig.add_subplot(gs[1, 2])\n",
        "confidence = y_pred_proba.max(axis=1)\n",
        "sns.histplot(confidence, kde=True, ax=ax)\n",
        "ax.set_title('Prediction Confidence Distribution')\n",
        "\n",
        "# Feature importance (if available)\n",
        "ax = fig.add_subplot(gs[2, :])\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_cols,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    sns.barplot(data=feature_importance, x='importance', y='feature', ax=ax)\n",
        "    ax.set_title('Feature Importances')\n",
        "\n",
        "plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print final report\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"\\nPerformance Summary:\")\n",
        "print(f\"  - Test Accuracy: {results[best_model_name]['test']:.4f}\")\n",
        "print(f\"  - AUC-ROC Score: {results[best_model_name]['auc']:.4f}\")\n",
        "print(f\"  - Cross-Val Score: {results[best_model_name]['cv_mean']:.4f} (+/- {results[best_model_name]['cv_std']:.4f})\")\n",
        "print(f\"  - F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "MGKjX4VBSOLZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}