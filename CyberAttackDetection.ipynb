{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxAIocPNzO+0JC8Jdt1nuW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Cyber-Attack-Detection/blob/main/CyberAttackDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FlowChart"
      ],
      "metadata": {
        "id": "2OYrAS-e0DNK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kki9cyway1lD",
        "outputId": "3e055ecc-3ccb-4d7c-ef1e-18ba444ffb4c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'flowchart.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "dot = Digraph(comment='Cyber Attack Detection Flowchart')\n",
        "dot.node('A', 'Start')\n",
        "dot.node('B', 'Define Problem & Scope')\n",
        "dot.node('C', 'Collect Dataset')\n",
        "dot.node('D', 'Preprocess Data')\n",
        "dot.node('E', 'Perform EDA')\n",
        "dot.node('F', 'Select Model')\n",
        "dot.node('G', 'Train Model')\n",
        "dot.node('H', 'Evaluate Model')\n",
        "dot.node('I', 'Optimize Model')\n",
        "dot.node('J', 'Deploy Model')\n",
        "dot.node('K', 'Monitor & Retrain')\n",
        "dot.node('L', 'End')\n",
        "\n",
        "dot.edges(['AB', 'BC', 'CD', 'DE', 'EF', 'FG', 'GH', 'HJ', 'JK', 'KL'])\n",
        "dot.edge('H', 'I', label='If performance poor')\n",
        "dot.edge('I', 'G', label='Retrain')\n",
        "\n",
        "dot.render('flowchart', format='png', view=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Path to the zip file\n",
        "zip_path = '/content/drive/MyDrive/network-intrusion-dataset.zip'\n",
        "extract_dir = '/content/cicids2017/'\n",
        "\n",
        "# Unzip the dataset\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(\"Dataset extracted successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Zip file not found at {zip_path}. Please check the path.\")\n",
        "    exit()\n",
        "\n",
        "# Step 1: Dynamically Find CSV Files\n",
        "def find_csv_files():\n",
        "    csv_files = []\n",
        "    for root, _, files in os.walk(extract_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.csv'):\n",
        "                csv_files.append(os.path.join(root, file))\n",
        "    print(\"Found CSV files:\")\n",
        "    for file in csv_files:\n",
        "        print(f\"  {file}\")\n",
        "    return csv_files\n",
        "\n",
        "csv_files = find_csv_files()\n",
        "if not csv_files:\n",
        "    print(\"Error: No CSV files found in\", extract_dir)\n",
        "    exit()\n",
        "\n",
        "# Step 2: Load and Combine Datasets\n",
        "def load_and_combine_data():\n",
        "    data_frames = []\n",
        "    for file in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(file, encoding='latin1', low_memory=False)\n",
        "            data_frames.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file}: {e}\")\n",
        "    if not data_frames:\n",
        "        print(\"Error: No data loaded.\")\n",
        "        exit()\n",
        "    combined_df = pd.concat(data_frames, ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "print(\"\\nLoading and combining datasets...\")\n",
        "df = load_and_combine_data()\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "# Step 3: Initial Data Inspection\n",
        "def inspect_data():\n",
        "    print(\"\\nStep 3: Initial Data Inspection\")\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nColumn names:\")\n",
        "    print(df.columns.tolist())\n",
        "    print(\"\\nData types:\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\nBasic statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    # Visualize columns\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.bar(range(len(df.columns)), [1] * len(df.columns))\n",
        "    plt.xticks(range(len(df.columns)), df.columns, rotation=90)\n",
        "    plt.title(\"Dataset Columns\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('columns.png')\n",
        "    plt.close()\n",
        "\n",
        "inspect_data()\n",
        "\n",
        "# Step 4: Check for Missing Values\n",
        "def check_missing_values():\n",
        "    print(\"\\nStep 4: Check for Missing Values\")\n",
        "    missing_values = df.isnull().sum()\n",
        "    print(\"\\nMissing values per column:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "\n",
        "    # Visualize missing values\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    missing_values[missing_values > 0].plot(kind='bar')\n",
        "    plt.title(\"Missing Values per Column\")\n",
        "    plt.xlabel(\"Columns\")\n",
        "    plt.ylabel(\"Number of Missing Values\")\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('missing_values.png')\n",
        "    plt.close()\n",
        "\n",
        "check_missing_values()\n",
        "\n",
        "# Step 5: Handle Missing Values\n",
        "def handle_missing_values():\n",
        "    print(\"\\nStep 5: Handle Missing Values\")\n",
        "    global df\n",
        "    # Replace inf values with NaN\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    # Impute numerical columns with median\n",
        "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
        "    # Verify no missing values remain\n",
        "    print(\"\\nMissing values after imputation:\")\n",
        "    print(df.isnull().sum().sum())\n",
        "\n",
        "handle_missing_values()\n",
        "\n",
        "# Step 6: Analyze Class Distribution\n",
        "def analyze_class_distribution():\n",
        "    print(\"\\nStep 6: Analyze Class Distribution\")\n",
        "    # Map labels to Attack (1) and Non-Attack (0)\n",
        "    df['Label'] = df[' Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
        "    class_counts = df['Label'].value_counts()\n",
        "    print(\"\\nClass distribution:\")\n",
        "    print(class_counts)\n",
        "\n",
        "    # Visualize class distribution\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.countplot(x='Label', data=df)\n",
        "    plt.title(\"Class Distribution (0: Non-Attack, 1: Attack)\")\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.savefig('class_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Detailed attack type distribution\n",
        "    attack_types = df[df['Label'] == 1][' Label'].value_counts()\n",
        "    print(\"\\nAttack types distribution:\")\n",
        "    print(attack_types)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    attack_types.plot(kind='bar')\n",
        "    plt.title(\"Distribution of Attack Types\")\n",
        "    plt.xlabel(\"Attack Type\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('attack_types_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "analyze_class_distribution()\n",
        "\n",
        "# Step 7: Feature Correlation Analysis\n",
        "def correlation_analysis():\n",
        "    print(\"\\nStep 7: Feature Correlation Analysis\")\n",
        "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    corr_matrix = df[numerical_cols].corr()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title(\"Correlation Matrix of Numerical Features\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('correlation_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "correlation_analysis()\n",
        "\n",
        "# Step 8: Feature Distribution Analysis\n",
        "def feature_distribution():\n",
        "    print(\"\\nStep 8: Feature Distribution Analysis\")\n",
        "    key_features = [' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets', ' Flow Bytes/s']\n",
        "    for feature in key_features:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(df[feature], bins=50, kde=True)\n",
        "        plt.title(f\"Distribution of {feature}\")\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.savefig(f'distribution_{feature.replace(\" \", \"_\")}.png')\n",
        "        plt.close()\n",
        "\n",
        "feature_distribution()\n",
        "\n",
        "# Step 9: Data Preprocessing\n",
        "def preprocess_data():\n",
        "    print(\"\\nStep 9: Data Preprocessing\")\n",
        "    global df\n",
        "    # Drop original label column\n",
        "    df = df.drop(' Label', axis=1)\n",
        "    # Normalize numerical features\n",
        "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    numerical_cols = [col for col in numerical_cols if col != 'Label']\n",
        "    scaler = StandardScaler()\n",
        "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
        "\n",
        "    # Convert features to text\n",
        "    def features_to_text(row):\n",
        "        text = \" \".join([f\"{col}:{row[col]}\" for col in numerical_cols])\n",
        "        return text\n",
        "\n",
        "    df['text'] = df.apply(features_to_text, axis=1)\n",
        "    print(\"\\nSample text representation:\")\n",
        "    print(df['text'].iloc[0])\n",
        "\n",
        "    # Save preprocessed dataset\n",
        "    df[['text', 'Label']].to_csv('preprocessed_cicids2017.csv', index=False)\n",
        "    print(\"\\nPreprocessed dataset saved as 'preprocessed_cicids2017.csv'\")\n",
        "\n",
        "preprocess_data()\n",
        "\n",
        "# Step 10: Summary of Preprocessed Data\n",
        "def summarize_preprocessed_data():\n",
        "    print(\"\\nStep 10: Summary of Preprocessed Data\")\n",
        "    print(\"\\nShape of preprocessed dataset:\")\n",
        "    print(df[['text', 'Label']].shape)\n",
        "    print(\"\\nSample data:\")\n",
        "    print(df[['text', 'Label']].head())\n",
        "\n",
        "summarize_preprocessed_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUCzavz63imk",
        "outputId": "140a2cbb-9a65-481c-a838-1626567acb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cpu\n",
            "Dataset extracted successfully.\n",
            "Found CSV files:\n",
            "  /content/cicids2017/Wednesday-workingHours.pcap_ISCX.csv\n",
            "  /content/cicids2017/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "  /content/cicids2017/Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "  /content/cicids2017/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "  /content/cicids2017/Monday-WorkingHours.pcap_ISCX.csv\n",
            "  /content/cicids2017/Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "  /content/cicids2017/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "  /content/cicids2017/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
            "\n",
            "Loading and combining datasets...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "extract_dir = '/content/cicids2017/'\n",
        "print(\"Listing contents of\", extract_dir)\n",
        "for root, dirs, files in os.walk(extract_dir):\n",
        "    print(f\"Directory: {root}\")\n",
        "    for file in files:\n",
        "        print(f\"  File: {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iqrPPdn5ihw",
        "outputId": "eea2630c-3cfd-4405-df95-7c33ca15fce7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing contents of /content/cicids2017/\n",
            "Directory: /content/cicids2017/\n",
            "  File: Wednesday-workingHours.pcap_ISCX.csv\n",
            "  File: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
            "  File: Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
            "  File: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
            "  File: Monday-WorkingHours.pcap_ISCX.csv\n",
            "  File: Tuesday-WorkingHours.pcap_ISCX.csv\n",
            "  File: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
            "  File: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n"
          ]
        }
      ]
    }
  ]
}